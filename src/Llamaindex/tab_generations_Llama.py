import streamlit as st
from llama_index.llms.groq import Groq
#from llama_index.core import Settings
from llama_index.core.query_engine import RetrieverQueryEngine

#LLM
llm_Llama = Groq(model = "llama3-70b-8192", temperature = 0.2) # request_timeout = 30.0
#Settings.llm = llm_Llama

import os 
import config as config
os.environ['GROQ_API_KEY'] = config.GROQ_API_KEY


def answer(retriever, llm, query):
    query_engine = RetrieverQueryEngine.from_args(
        retriever = retriever, 
        llm = llm,   
    )
    return query_engine.query(query)

def tab_generation_Lla():
    st.header('Generation')
    st.write("""In this tab we generate the response with an LLM model, in this case we will use the Llama 3-70b model. For each
    question we can compare the answer generated by the model with the ground truth and see if the answer is really
    generated from the retrieval. Â """)
    
    if st.button("Run", key = "Generation_Llama"):
        questions = st.session_state['editable_questions'] 
        ground_truths = st.session_state['editable_ground_truths']
        context_Llama = st.session_state['context_Llama']        
        retriever_Llama = st.session_state['retriever_Llama']

        answers_Llama = []
        for i in range(len(questions)): 
            response = answer(retriever_Llama, llm_Llama, questions[i])
            st.write(f"**Query:** {questions[i]}")
            st.write(f"**Ground Truth:** {ground_truths[i]}")
            st.write(f"**Answer:** {response}")
            st.write(f"**Retrieval:** {context_Llama[i]}")
            st.write('---')
            answers_Llama.append(response)

        st.session_state['answers_Llama'] = answers_Llama

