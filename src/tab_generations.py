import streamlit as st
from tab_retrieval import embedding, retriever
from langchain_groq import ChatGroq
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

import os 
import config
os.environ['GROQ_API_KEY'] = config.GROQ_API_KEY
os.environ['OPENAI_API_KEY'] = config.OPENAI_API_KEY

llm = ChatGroq(temperature = 0.2, model_name = "Llama3-70b-8192")

PROMPT_TEMPLATE = """You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \
If you don't know the answer, just say that you don't know. Try to provide a complete but concise answer.
Question: {question}
Context:{context}

Answer: 
"""
prompt_template_aux = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)

def chains(query, retriever, prompt_template = prompt_template_aux, llm = ChatOpenAI()):
    chain = (
        {
            'context': retriever, 
            'question': RunnablePassthrough()
            }
            | prompt_template
            | llm
            | StrOutputParser()
            )
    res = chain.invoke(query)
    return res


def tab_generation():
    st.header('Generation')
    st.write("""In this tab we generate the response with an LLM model, in this case we will use the Llama 3-70b model. For each
    question we can compare the answer generated by the model with the ground truth and see if the answer is really
    generated from the retrieval. Â """)
    
    if st.button("Run", key = "Generation"):
        questions = st.session_state['editable_questions'] 
        ground_truths = st.session_state['editable_ground_truths']
        context = st.session_state['context']        
        embed_model = st.session_state['embed_model']
        chunks_gen = st.session_state['chunks_gen']
        retrieval_model = st.session_state['retrieval_model']

        embeddings1 = embedding(embed_model)
        retrieval = retriever(chunks = chunks_gen, model = retrieval_model, embeddings = embeddings1)
        
        answers = []
        for i in range(len(questions)): 
            generate_answer = chains(questions[i], retrieval, llm = llm)
            st.write(f"**Query:** {questions[i]}")
            st.write(f"**Ground Truth:** {ground_truths[i]}")
            st.write(f"**Answer:** {generate_answer}")
            st.write(f"**Retrieval:** {context[i]}")
            st.write('---')
            answers.append(generate_answer)

        st.session_state['answer'] = answers



