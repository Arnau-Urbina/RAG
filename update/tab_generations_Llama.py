import streamlit as st
from llama_index.core.query_engine import RetrieverQueryEngine
from Llamaindex.tab_retrieval_Llama import load_llm

def answer(retriever, llm, query):
    query_engine = RetrieverQueryEngine.from_args(
        retriever = retriever, 
        llm = llm,   
    )
    return query_engine.query(query)

def tab_generation_Lla():
    st.header('Generation')
    st.write("""In this tab we generate the response with an LLM model, in this case we will use the Llama 3-70b model. For each
    question we can compare the answer generated by the model with the ground truth and see if the answer is really
    generated from the retrieval. Â """)
    
    if st.button("Run", key = "Generation_Llama"):
        questions = st.session_state['editable_questions'] 
        ground_truths = st.session_state['editable_ground_truths']
        context_Llama = st.session_state['context_Llama']        
        retriever_Llama = st.session_state['retriever_Llama']
        llm_model_lla = st.session_state['llm_model_lla']
        llm_Llama = load_llm(llm_model_lla)

        answers_Llama = []
        for i in range(len(questions)): 
            response = answer(retriever_Llama, llm_Llama, questions[i])
            st.write(f"**Query:** {questions[i]}")
            st.write(f"**Ground Truth:** {ground_truths[i]}")
            st.write(f"**Answer:** {response}")
            st.write(f"**Retrieval:** {context_Llama[i]}")
            st.write('---')
            answers_Llama.append(response)

        st.session_state['answers_Llama'] = answers_Llama

