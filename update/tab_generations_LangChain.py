import streamlit as st
from Langchain.tab_retrieval_LangChain import embedding, retrievers
from langchain.prompts import ChatPromptTemplate
from langchain.schema.runnable import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from Langchain.tab_retrieval_LangChain import load_llm #llm

PROMPT_TEMPLATE = """You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. \
If you don't know the answer, just say that you don't know. Try to provide a complete but concise answer.
Question: {question}
Context:{context}

Answer: 
"""
prompt_template_aux = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)

def chains(query, retrievers, llm, prompt_template = prompt_template_aux):
    chain = (
        {
            'context': retrievers, 
            'question': RunnablePassthrough()
        }
        | prompt_template
        | llm
        | StrOutputParser()
    )
    res = chain.invoke(query)
    return res


def tab_generation():
    st.header('Generation')
    st.write("""In this tab we generate the response with an LLM model, in this case we will use the Llama 3-70b model. For each
    question we can compare the answer generated by the model with the ground truth and see if the answer is really
    generated from the retrieval. Â """)
    
    if st.button("Run", key = "Generation_Lang"):
        questions = st.session_state['editable_questions'] 
        ground_truths = st.session_state['editable_ground_truths']
        context = st.session_state['context']        
        embed_model = st.session_state['embed_model']
        chunks_gen = st.session_state['chunks_gen']
        retrieval_model = st.session_state['retrieval_model']
        llm_model = st.session_state['llm_model']
        llm = load_llm(llm_model)

        embeddings1 = embedding(embed_model)
        retrieval = retrievers(chunks = chunks_gen, model = retrieval_model, embeddings = embeddings1)
        
        answers = []
        for i in range(len(questions)): 
            generate_answer = chains(questions[i], retrieval, llm = llm)
            st.write(f"**Query:** {questions[i]}")
            st.write(f"**Ground Truth:** {ground_truths[i]}")
            st.write(f"**Answer:** {generate_answer}")
            st.write(f"**Retrieval:** {context[i]}")
            st.write('---')
            answers.append(generate_answer)

        st.session_state['answer'] = answers




